# @package _global_
# Configuration for unplug_charger task with segmentation-based attention
defaults:
  - pointflowmatch_so3_attention

# Task-specific configuration
env_config:
  task_name: unplug_charger

# Dataset configuration with task-specific object IDs
dataset:
  object_ids: [31, 34, 35, 92]  # Specific to unplug_charger task
  # You may need to update these IDs based on analyze_collected_masks.py output
  
# Training hyperparameters that might need tuning
model:
  lambda_attention: 0.1  # Reduced from 0.5 to minimize attention overfitting
  
# Reduce attention capacity to prevent overfitting
backbone:
  attention_hidden_dim: 32  # Reduced from 256 to prevent overfitting
  
# Optional: Start with lower learning rate for fine-tuning
trainer:
  learning_rate: 3e-5  # Lower than default 3e-5

# WandB configuration
wandb:
  name: "attention_lambda0.1_dropout0.3_hidden32"  # Custom run name
  # project: "pointflowmatch"  # Optional: override project name
  # tags: ["attention", "dropout", "reduced_capacity"]  # Optional: add tags